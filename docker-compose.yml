# Remove the version: '3.8' line as per the warning
services:
  # Your FastAPI Application Service
  app:
    build: . # Build the image using the SIMPLIFIED Dockerfile
    image: realtime-voice-chat:latest # Name the image built by 'build:'
    container_name: realtime-voice-chat-app
    ports:
      - "8000:8000"
    environment:
      # Point to the 'ollama' service
      - OLLAMA_BASE_URL=http://ollama:11434
      # --- Other App Environment Variables ---
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - MAX_AUDIO_QUEUE_SIZE=${MAX_AUDIO_QUEUE_SIZE:-50}
      - HF_HOME=/home/appuser/.cache/huggingface
      - TORCH_HOME=/home/appuser/.cache/torch
    # command: ["python", "-m", "uvicorn", "server:app", "--host", "0.0.0.0", "--port", "8000", "--reload", "--reload-dir", "/app/code"]
    volumes:
       # Optional: Mount code for live development
       - ./code:/app/code
       # Mount cache directories
       - huggingface_cache:/home/appuser/.cache/huggingface
       - torch_cache:/home/appuser/.cache/torch
      #  - services-data/app/cache:/home/appuser/.cache
    depends_on:
       - ollama
    restart: unless-stopped
    networks:
      - default
    healthcheck:
      # Check if the Ollama API is responsive
      test: ["CMD", "wget", "--quiet", "--spider", "--tries=1", "--timeout=10", "http://ollama:11434/api/tags"]
      interval: 15s
      timeout: 10s
      retries: 12
      start_period: 45s # Give it time to start        

  # ollama:
  #   image: ghcr.io/ggml-org/llama.cpp:server
  #   container_name: realtime-voice-chat-ollama
  #   environment:
  #     - TZ=America/Sao_Paulo
  #     - N_GPU_LAYERS=0
  #     - MODEL=Phi-3-mini-4k-instruct-Q4_K_M.gguf
  #     - N_GPU_LAYERS=0
  #   volumes:
  #     - ./services-data/llama/models:/models:ro
  #     - ./services-data/llama/models/Phi-3-mini-4k-instruct-Q4_K_M.gguf:/models/Phi-3-mini-4k-instruct-Q4_K_M.gguf:ro
  #   command: >
  #     -m /models/Phi-3-mini-4k-instruct-Q4_K_M.gguf
  #     --host 0.0.0.0
  #     --port 11434
  #     --ctx-size 8192
  #     --parallel 2
  #     --mlock
  #   ports:
  #     - "11434:11434"
  #   healthcheck:
  #     # Check if the Ollama API is responsive
  #     test: ["CMD", "wget", "--quiet", "--spider", "--tries=1", "--timeout=10", "http://localhost:11434/api/tags"]
  #     interval: 15s
  #     timeout: 10s
  #     retries: 12
  #     start_period: 45s # Give it time to start     










    # healthcheck:
    #   test: ["CMD-SHELL", "curl -fsS http://localhost:8080/v1/models || exit 1"]
    #   interval: 10s
    #   timeout: 5s
    #   retries: 10
    # restart: unless-stopped

  # Ollama Server Service (Using Official Image)
  ollama:
    # --- Use the official Ollama image ---
    image: ollama/ollama:latest
    container_name: realtime-voice-chat-ollama
    # --- No 'build:' section needed here ---
    # command: ["ollama", "serve"] # Usually the default command/entrypoint
    # command: >
    #   -m /models/Phi-3-mini-4k-instruct-Q4_K_M.gguf
    #   --host 0.0.0.0
    #   --port 8080
    #   --ctx-size 8192
    #   --parallel 2
    #   --mlock
    ports:
      - "11434:11434"
    volumes:
      # Persist Ollama models and data
      # - ollama_data:/root/.ollama
      - ./services-data/ollama:/root/.ollama
    environment:
      # OLLAMA_MODELS might be useful if needed, points inside volume
      - OLLAMA_MODELS=/root/.ollama/models
      - LLM_START_PROVIDER=ollama
      - LLM_START_MODEL=hf.co/bartowski/Llama-3.2-3B-Instruct-GGUF:Q8_0
    healthcheck:
      # Check if the Ollama API is responsive
      test: ["CMD", "wget", "--quiet", "--spider", "--tries=1", "--timeout=10", "http://localhost:11434/api/tags"]
      interval: 15s
      timeout: 10s
      retries: 12
      start_period: 45s # Give it time to start
    restart: unless-stopped
    networks:
      - default

networks:
  default:
    driver: bridge

# Define named volumes for persistent data
volumes:
  ollama_data:
    driver: local
  huggingface_cache:
    driver: local
  torch_cache:
    driver: local